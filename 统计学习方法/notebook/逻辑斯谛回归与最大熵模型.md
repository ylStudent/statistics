# 第6章 逻辑斯蒂回归与最大熵模型
## 6.1 逻辑斯蒂回归模型
### 6.1.1 逻辑斯蒂分布
定义 6.1（逻辑斯谛分布）     
设$X$是连续随机变量，$X$服从逻辑斯谛分布是指X 具有下列分布函数和密度函数：
$$ F(x) = P(X<= x) = \frac{1}{1+exp^{-(x-\mu)/{\gamma}}}$$
$$ f(x) = F'(x) = \frac{e^{-(x-\mu)/{\gamma}}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}$$
其中$\mu$为位置参数，$\gamma>0$为形状参数。
其图形是一个S形的曲线，该曲线以点$(\mu, \frac{1}{2})$为中心对称，即满足
$$F(-x+\mu) - \frac{1}{2} = -F(x+\mu) + \frac{1}{2}$$
### 6.1.2 二项逻辑斯蒂回归模型
（逻辑斯蒂回归模型）二项逻辑斯谛回归模型是如下的条件概率 
分布：
$$ P(Y=1|x) = \frac{exp(wx+b)}{1+exp(wx+b)}$$
$$ P(Y=0|x) = \frac{1}{1+exp(wx+b)}$$
考察逻辑斯蒂回归模型的特点：
如果事件的发生的概率是$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数概率或logit函数是
$$ logit(p) = log(\frac{p}{1-p}) $$
那么有
$$ log\frac{P(Y=1|x)}{1-P(Y=1|x)} = wx$$
在逻辑斯谛回归模型中，输出Y 1的对数几率是输入 x 的线性函数. 或者说，输出Y 1的对数几率是由输入 x 的线性函数表示的模型.
### 6.1.3 模型参数估计
设： $P(Y=1|x) = \pi(x)$, $P(Y=0|x) = 1-\pi(x)$
似然函数为
$$ \prod_{i=0}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i} $$
对数似然函数为
$$L(w) = \sum_{i=1}^{N}[y_i(w*x_i)-log(1+exp(wx_i)]$$
该问题变为以对数似然函数为目标函数的最优化问题，通常采用的方法是梯度下降及拟牛顿法。
### 6.1.4 多项逻辑斯谛回归
假设离散型随机变量$Y$的取值集合为$\{1, 2, .., K\}$，那么多项逻辑斯蒂回归的模型为：
$$P(Y=k|x) = \frac{exp(w_kx)}{1+\sum_{k=1}^{K-1}exp(w_kx)}, k=1, 2, ..., K-1$$
$$P(Y=K|x) = \frac{1}{1+\sum_{k=1}^{K-1}exp(w_kx)}, k = K$$

## 6.2 最大熵模型
### 6.2.1 最大熵原理
假设离散随机变量$X$的概率分布是$P(X)$，则其熵是
$$H(P) = -\sum{x}P(x)logP(x)$$
熵满足如下不等式：
$$ 0 <= H(P) <= log|X| $$
其中$|X|$是$X$的取值个数，当且仅当$X$的分布是均匀分布时右边的等号成立。
### 6.2.2 最大熵模型的定义
给定一个训练集：
$$ T = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$$
学习的目标是用最大熵原理选择最好的分类模型．
确定其联合分布和边缘分布$P(X)$的经验分布
$$ \hat{P}(X=x, Y=y) = \frac{v(X=x, Y=y)}{N}$$
$$ \hat{P}(X=x) = \frac{v(X=x)}{N}$$
其中 $v(X=x, Y=y)$，$v(X=x)$表示频数  
feature function的定义为
$$f(x, y) = 
\begin{cases}
1_{\quad x, y符合某一事实}\\
0_{\quad 否则}
\end{cases}
$$
特征函数$f(x,y)$关于经验分布$\hat{P}(X,Y)$的期望值，用$E_{\hat{P}}(f)$表示
$$E_{\hat{P}}(f)=\sum_{x,y}\hat{P}(x,y)f(x,y)$$
特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\hat{P}(X)$的期望值，用$E_P(f)$表示
$$E_{P}(f)=\sum_{x,y}\hat{P}(x)P(y|x)f(x,y)$$
如果模型能够获取训练集数据中的信息，那么就可以假设这两个期望值相等，即
$$E_{P}(f) = E_{\hat{P}}(f)$$
假设满足所有约束条件的模型集合为
$$ C \equiv \{P \in p | E_p(f_i) = E_{\hat{P}}(f_i), i=1, 2,..., n\}$$
定义在条件概率分布$P(Y|X)$上的条件熵为
$$ H(P) = -\sum_{x,y}\hat{P}(x)P(y|x)logP(y|x)$$
则模型集合$C$中条件熵$H(P)$最大的模型称为最大熵模型．式中的对数为自然 对数．

### 6.2.3 最大熵模型的学习
对于给定的训练数据集$T={(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)}$以及特征函数$f_i(x,y)i=1,2,...,n$，最大熵模型的学习等价于约束最优化问题：
$$min -\quad H(P) = -\sum_{x,y}\hat{P}(x)P(y|x)logP(y|x) \quad P \in C$$
$$s.t \quad E_P(f_i) = E_{\hat{P}}(f_i)$$
$$ \sum_y P(y|x) = 1 $$


$$ hello $$
$$ E = mc^2 $$
$$ what do you like ? $$
$$ what's your meaning $$

$$ how do you do $$

