# 决策树
## 5.1 决策树模型与学习
定义：分类决策树模型是一种描述对实例进行分类的树形结
构．决策树由结点（node）和有向边（directed edge）组成．结点有两种类型：内部结点（internal node）和叶结点（leaf node）．内部结点表示一个特征或属性，叶结点表示一个类。

## 5.2 特征选择
### 5.2.1 特征选择问题
特征选择在于选取对训练数据具有分类能力的特征．这样可以提高决策树学
习的效率．如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的．经验上扔掉这样的特征对决策树学习的精度影响不大．通常特征选择的准则是信息增益或信息增益比。
### 5.2.2 信息增益
在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量．设X 是一个取有限个值的离散随机变量，其概率分布为
$$P(X=x_i)=p_i, i=1, 2, .., n$$
而随机变量$X$的熵定义为
$$H(X) = -\sum_{i=1}^{n}p_ilogp_i$$
由定义可知，熵只依赖于$X$ 的分布，而与$X$的取值无关，所以也可将 $X$ 的熵记作$H(p)$，即
$$H(p) = \sum_{i=1}^{n}p_ilogp_i$$
熵越大，随机变量不确定性就越大。

**信息增益**  
特征A对训练数据集D的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
$$g(D,A) = H(D) - H(D|A)$$

一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息（mutual information）．决策树学习中的信息增益等价于训练数据集中类与特征的互信息．

### 5.2.3 信息增益比
特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集 D 的经验熵$H(D)$之比：
$$ g_r(D,A) = \frac{g(D,A)}{H(D)}$$

## 5.3 决策树的生成
### 5.3.1 ID3算法
ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。
### C4.5 的生成算法
C4.5 算法与 ID3 算法相似，C4.5 算法对 ID3 算法进行了改进．C4.5 在生成的过程中，用信息增益比来选择特征。

## 5.4 决策树的剪枝
在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）．具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。

决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现．
设树T的叶节点个数为$|T|$，t是树$T$的叶节点，该叶结点有 $N_t$ 个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,...3$ $H_{Tt}$为叶结点t上的经验熵，为参数$\theta >= 0$，则决策树学习的损失函数可以定义为$$C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T) + \alpha|T| $$
其中经验熵为
$$ H_t(T) = -\sum_{k}\frac{N_{tk}}{N_t}log{\frac{N_{tk}}{N_t}}$$
这时有
$$ C_{\alpha}(T) = C(T) + \alpha|T|$$

剪枝，就是当$\alpha$确定时，选择损失函数最小的模型，即损失函数最小的子树．当$\alpha$值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好．损失函数正好表示了对两者的平衡.   
设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$ ，其对应的损失函数值分别是$C_{\alpha}(T_A)$与$C_{\alpha}(T_B)$，如果 
$$C_{\alpha}(T_A) <= C_{\alpha}(T_B)$$
则进行剪枝，即将父结点变为新的叶结点．
## 5.5 CART算法
CART算法假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支．  
CART 算法由以下两步组成：
1. 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准
### 5.5.1 CART 生成
假设 X 与Y 分别为输入和输出变量，并且Y是连续变量，给定训练数据集
$$D=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
考虑如何生成回归树．  
**算法**：  
输入：训练数据集 D ；  
输出：回归树$f(x)$  
在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：  
1. 选择最优切分变量 j 与切分点 s ，求解$$ min_{js}[min_{c_1 }\sum_{x_1 \in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]$$遍历变量$j$ ，对固定的切分变量$j$扫描切分点$s$，选择上式达到最小值的对$(j,s)$．  
2. 用选定的对$(j,s)$划分区域并决定相应的输出值：$$R_1(j,s) = \{x|x^{(j)} <= s\}, R_2(j,s)=\{x|x^{(j)} > s\}$$ $$ \hat{c_m} = \frac{1}{N_m}\sum_{x_i \in R_m(j,s)}y_i, x \in R_m, m=1,2$$
3. 继续对两个子区域调用步骤 (1)，(2)，直至满足停止条件．
4. 即将输入空间划分为M个区域$R_1, R_2,..., R_M$
$$ f(x) = \sum_{m=1}^{M}\hat{c_m}I(x \in R_m)$$
