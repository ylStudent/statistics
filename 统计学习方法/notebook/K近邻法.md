# K近邻法
k 近邻法（k-nearest neighbor，k-NN）是一种基本分类与回归方法。
## 3.1 K近邻算法
给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k 个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。  
输入：
$$ T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
其中，$x_i$为特征向量，$y_i$为类别，$i=1,2,..,N$
## 3.2 k近邻模型
### 3.2.1 模型

### 3.2.2 距离度量
使用的距离可以是欧氏距离，曼哈顿距离$(L_p)$距离或者Minkowski 距离（Minkowski distance）。
### 3.2.3 k值的选择
如果选择较小的k值，近似误差会减小，但是估计误差会增大，对近邻点敏感，如果恰好是噪声，预测出错，易发生过拟合。

较大的k值，近似误差增大，估计误差减小，模型简单。
### 3.2.4 分类决策规则
多数表决规则等价于经验最小化

## 3.3 k近邻法的实现：kd树
为了提高k 近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数．具体方法很多，因此引入kd树。
### 3.3.1 构造kd树

